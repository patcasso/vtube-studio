import os
import asyncio
import websockets
import json
import time
import pygame
import librosa
import numpy as np
import requests
import aiohttp
from dotenv import load_dotenv

load_dotenv()
VTUBE_STUDIO_AUTH_TOKEN = os.environ.get("VTUBE_STUDIO_AUTH_TOKEN")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
ELEVENLABS_API_KEY = os.environ.get("ELEVENLABS_API_KEY")
# ELEVENLABS_VOICE_ID = os.environ.get("ELEVENLABS_VOICE_ID_JYP")  # JYP voice
ELEVENLABS_VOICE_ID = os.environ.get("ELEVENLABS_VOICE_ID_OHW")  # OHW voice


# Function to get a response from OpenAI API
async def get_ai_response(user_input):
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {OPENAI_API_KEY}",
    }
    data = {
        "model": "gpt-4",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": user_input},
        ],
        "max_tokens": 300,
    }

    async with aiohttp.ClientSession() as session:
        async with session.post(url, headers=headers, json=data) as response:
            if response.status == 200:
                response_json = await response.json()
                ai_response = response_json["choices"][0]["message"]["content"].strip()
                print(f"\nAI Response: {ai_response}")
                return ai_response
            else:
                error_text = await response.text()
                print(f"Error from OpenAI API: {error_text}")
                return "Sorry, I couldn't process your request."


# Function to convert text to speech using ElevenLabs
async def text_to_speech(text):
    tts_start_time = time.time()  # TTS ì‹œì‘ ì‹œê°„

    url = f"https://api.elevenlabs.io/v1/text-to-speech/{ELEVENLABS_VOICE_ID}"
    headers = {
        "Accept": "audio/mpeg",
        "Content-Type": "application/json",
        "xi-api-key": ELEVENLABS_API_KEY,
    }
    data = {
        "text": text,
        "model_id": "eleven_multilingual_v2",
        "voice_settings": {"stability": 1, "similarity_boost": 1},
    }

    print("Requesting speech from ElevenLabs...")
    async with aiohttp.ClientSession() as session:
        async with session.post(url, headers=headers, json=data) as response:
            if response.status == 200:
                # Save the audio file
                audio_path = "./audio/output.mp3"
                os.makedirs(os.path.dirname(audio_path), exist_ok=True)
                with open(audio_path, "wb") as f:
                    f.write(await response.read())

                tts_end_time = time.time()  # TTS ì¢…ë£Œ ì‹œê°„
                tts_duration = tts_end_time - tts_start_time
                print(f"Speech saved to {audio_path}")
                print(f"â±ï¸ TTS ì²˜ë¦¬ ì‹œê°„: {tts_duration:.2f}ì´ˆ")

                return audio_path
            else:
                error_text = await response.text()
                print(f"Error from ElevenLabs API: {error_text}")
                return None


async def chat_with_character(websocket):
    # Initialize conversation history
    conversation_history = [
        {"role": "system", "content": "You are a helpful assistant."}
    ]

    while True:
        print("\nğŸ’¬ Enter your message (or type 'exit' to return to menu):")
        user_input = input("> ")

        if user_input.lower() == "exit":
            return

        total_start_time = time.time()  # ì „ì²´ ì²˜ë¦¬ ì‹œì‘ ì‹œê°„

        # Add user message to conversation history
        conversation_history.append({"role": "user", "content": user_input})

        # Step 1 & 2: Get AI response from OpenAI API with full conversation history
        llm_start_time = time.time()  # LLM ì²˜ë¦¬ ì‹œì‘ ì‹œê°„

        url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {OPENAI_API_KEY}",
        }
        data = {"model": "gpt-4", "messages": conversation_history, "max_tokens": 300}

        async with aiohttp.ClientSession() as session:
            async with session.post(url, headers=headers, json=data) as response:
                if response.status == 200:
                    response_json = await response.json()
                    ai_response = response_json["choices"][0]["message"][
                        "content"
                    ].strip()

                    llm_end_time = time.time()  # LLM ì²˜ë¦¬ ì¢…ë£Œ ì‹œê°„
                    llm_duration = llm_end_time - llm_start_time
                    print(f"\nAI Response: {ai_response}")
                    print(f"â±ï¸ LLM ì‘ë‹µ ì‹œê°„: {llm_duration:.2f}ì´ˆ")

                    # Add AI response to conversation history
                    conversation_history.append(
                        {"role": "assistant", "content": ai_response}
                    )

                    # Step 3 & 4: Convert AI response to speech using ElevenLabs
                    tts_start_time = (
                        time.time()
                    )  # TTS ì‹œì‘ ì‹œê°„ (í•¨ìˆ˜ ë‚´ì—ì„œë„ ì¸¡ì •í•˜ì§€ë§Œ ì—¬ê¸°ì„œë„ ì¸¡ì •)
                    audio_path = await text_to_speech(ai_response)
                    tts_end_time = time.time()  # TTS ì¢…ë£Œ ì‹œê°„
                    tts_duration = tts_end_time - tts_start_time

                    if audio_path:
                        # Step 5 & 6: Play the audio with lip-sync
                        print(f"Playing response with synchronized mouth movements...")
                        lipsync_start_time = time.time()  # ë¦½ì‹±í¬ ì‹œì‘ ì‹œê°„
                        await play_audio_with_mouth_sync(websocket, audio_path)
                        lipsync_end_time = time.time()  # ë¦½ì‹±í¬ ì¢…ë£Œ ì‹œê°„
                        lipsync_duration = lipsync_end_time - lipsync_start_time
                        print(f"â±ï¸ ë¦½ì‹±í¬ ì¬ìƒ ì‹œê°„: {lipsync_duration:.2f}ì´ˆ")

                        # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
                        total_end_time = time.time()
                        total_duration = total_end_time - total_start_time
                        print(f"â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_duration:.2f}ì´ˆ")

                        # ì‹œê°„ ìš”ì•½
                        print("\nâ±ï¸ ì²˜ë¦¬ ì‹œê°„ ìš”ì•½:")
                        print(f"   - LLM ì‘ë‹µ: {llm_duration:.2f}ì´ˆ")
                        print(f"   - TTS ë³€í™˜: {tts_duration:.2f}ì´ˆ")
                        print(f"   - ë¦½ì‹±í¬ ì¬ìƒ: {lipsync_duration:.2f}ì´ˆ")
                        print(f"   - ì´ ì†Œìš” ì‹œê°„: {total_duration:.2f}ì´ˆ")

                        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œê¹…
                        print("\nğŸ—‚ï¸ Conversation History:")
                        for message in conversation_history:
                            role = message["role"]
                            content = message["content"]
                            if role == "system":
                                print(f"ğŸ› ï¸ system prompt: {content}")
                            elif role == "user":
                                print(f"ğŸ§‘ user: {content}")
                            elif role == "assistant":
                                print(f"ğŸ¤– assistant: {content}")
                    else:
                        print(
                            "Failed to generate speech. Check your API keys and network connection."
                        )
                else:
                    error_text = await response.text()
                    print(f"Error from OpenAI API: {error_text}")
                    print("Sorry, I couldn't process your request.")


# Modified read_volume function to return both times and values
def read_volume(audio_file_path):
    analysis_start_time = time.time()  # ì˜¤ë””ì˜¤ ë¶„ì„ ì‹œì‘ ì‹œê°„

    # Load the audio file
    y, sr = librosa.load(audio_file_path)

    # Calculate the root mean square (RMS) energy for each frame
    hop_length = 512  # Default hop length in librosa
    frame_length = 2048  # Default frame length

    rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]

    # Get the time for each frame
    times = librosa.times_like(rms, sr=sr, hop_length=hop_length)

    # Normalize the RMS values to a 0-100 range
    normalized_rms = np.interp(rms, (0, rms.max()), (0, 1))
    mouth_values = normalized_rms * 3

    analysis_end_time = time.time()  # ì˜¤ë””ì˜¤ ë¶„ì„ ì¢…ë£Œ ì‹œê°„
    analysis_duration = analysis_end_time - analysis_start_time
    print(f"â±ï¸ ì˜¤ë””ì˜¤ ë¶„ì„ ì‹œê°„: {analysis_duration:.2f}ì´ˆ")

    return times, mouth_values


# Function to play audio and control mouth movement simultaneously
async def play_audio_with_mouth_sync(websocket, audio_path):
    # 1. Analyze audio first to get timing and mouth values
    print("Analyzing audio...")
    times, mouth_values = read_volume(audio_path)

    # 2. Create custom parameter if needed
    param_start_time = time.time()  # íŒŒë¼ë¯¸í„° ìƒì„± ì‹œì‘ ì‹œê°„
    creation_request = {
        "apiName": "VTubeStudioPublicAPI",
        "apiVersion": "1.0",
        "requestID": "ParamCreate_MouthOpenAmount",
        "messageType": "ParameterCreationRequest",
        "data": {
            "parameterName": "MouthOpenAmount",
            "explanation": "Controls the amount of mouth open from audio volume.",
            "min": 0,
            "max": 100,
            "defaultValue": 0,
        },
    }
    print("\nğŸ“Œ Creating custom tracking parameter 'MouthOpenAmount'...")
    await send_api_request(creation_request, websocket)
    param_end_time = time.time()  # íŒŒë¼ë¯¸í„° ìƒì„± ì¢…ë£Œ ì‹œê°„
    param_duration = param_end_time - param_start_time
    print(f"â±ï¸ íŒŒë¼ë¯¸í„° ìƒì„± ì‹œê°„: {param_duration:.2f}ì´ˆ")

    # 3. Prepare for synchronized playback
    playback_start_time = time.time()  # ì¬ìƒ ì¤€ë¹„ ì‹œì‘ ì‹œê°„
    print("Preparing for synchronized playback...")
    pygame.mixer.init()
    sound = pygame.mixer.Sound(audio_path)

    # 4. Set up timing variables
    frame_count = len(times)
    current_frame = 0

    # 5. Start audio playback
    print("Starting audio playback and mouth animation...")
    sound.play()
    start_time = time.time()
    print(f"â±ï¸ ì¬ìƒ ì¤€ë¹„ ì‹œê°„: {start_time - playback_start_time:.2f}ì´ˆ")

    # 6. Main synchronization loop
    while current_frame < frame_count:
        # Calculate what frame we should be on based on elapsed time
        elapsed_time = time.time() - start_time
        target_frame = 0

        # Find the appropriate frame for the current time
        while target_frame < frame_count and times[target_frame] < elapsed_time:
            target_frame += 1

        if target_frame >= frame_count:
            break

        # Only send updates when we move to a new frame
        if target_frame > current_frame:
            current_frame = target_frame
            mouth_value = mouth_values[current_frame]

            # Apply a scaling factor to make movements more pronounced
            # Adjust the multiplier (20) as needed for your character
            scaled_value = min(mouth_value * 20, 100)

            # Send the parameter to VTube Studio
            api_request = {
                "apiName": "VTubeStudioPublicAPI",
                "apiVersion": "1.0",
                "requestID": "MouthSync",
                "messageType": "InjectParameterDataRequest",
                "data": {
                    "faceFound": True,
                    "mode": "set",
                    "parameterValues": [
                        {"id": "MouthOpenAmount", "value": scaled_value},
                    ],
                },
            }
            await send_api_request(api_request, websocket, silent=True)

        # Small sleep to prevent flooding with requests
        await asyncio.sleep(0.01)

    # 7. Wait for audio to finish and reset mouth
    remaining_time = sound.get_length() - elapsed_time
    if remaining_time > 0:
        await asyncio.sleep(remaining_time)

    # 8. Reset mouth to closed position
    reset_request = {
        "apiName": "VTubeStudioPublicAPI",
        "apiVersion": "1.0",
        "requestID": "ResetMouth",
        "messageType": "InjectParameterDataRequest",
        "data": {
            "faceFound": True,
            "mode": "set",
            "parameterValues": [
                {"id": "MouthOpenAmount", "value": 0},
            ],
        },
    }
    await send_api_request(reset_request, websocket)
    print("Audio playback and mouth animation completed.")


# Silent version of send_api_request for high-frequency updates
async def send_api_request(request, websocket, silent=False):
    api_request_json = json.dumps(request)
    await websocket.send(api_request_json)
    response = await websocket.recv()
    if not silent:
        response_json = json.loads(response)
        print(json.dumps(response_json, indent=4))
        return response_json
    return json.loads(response)


# Your existing functions
async def request_auth_token(websocket):
    token_request = {
        "apiName": "VTubeStudioPublicAPI",
        "apiVersion": "1.0",
        "requestID": "TokenRequestID",
        "messageType": "AuthenticationTokenRequest",
        "data": {
            "pluginName": "VtsTestPlugin",
            "pluginDeveloper": "Patcasso",
        },
    }
    print("\nğŸ”‘ Requesting authentication token...")
    response = await send_api_request(token_request, websocket)

    if response["messageType"] == "AuthenticationTokenResponse":
        token = response["data"]["authenticationToken"]
        print(f"\nâœ… Token received: {token}")
        return token
    elif response["messageType"] == "APIError":
        print(f"\nâŒ Error: {response['data']['message']}")
        return None
    else:
        print("\nâŒ Unexpected response.")
        return None


async def authenticate_with_token(websocket, token):
    auth_request = {
        "apiName": "VTubeStudioPublicAPI",
        "apiVersion": "1.0",
        "requestID": "AuthRequestID",
        "messageType": "AuthenticationRequest",
        "data": {
            "pluginName": "VtsTestPlugin",
            "pluginDeveloper": "Patcasso",
            "authenticationToken": token,
        },
    }
    print("\nğŸ” Authenticating with token...")
    response = await send_api_request(auth_request, websocket)

    if (
        response["messageType"] == "AuthenticationResponse"
        and response["data"]["authenticated"]
    ):
        print("\nâœ… Plugin authenticated!")
        return True
    else:
        print("\nâŒ Authentication failed.")
        return False


# Modified main function
async def main():
    uri = "ws://localhost:8001"

    async with websockets.connect(uri) as websocket:
        print("1. ì…ë ¥ëœ í† í° ì‚¬ìš©")
        print("2. ìƒˆë¡œ í† í° ìš”ì²­ (íŒì—… ëœ¸)")
        token_option = input("ì„ íƒ (1/2): ")

        token = None
        if token_option == "1":
            # token = input("ì €ì¥ëœ í† í° ì…ë ¥: ")
            token = VTUBE_STUDIO_AUTH_TOKEN
        elif token_option == "2":
            token = await request_auth_token(websocket)
        else:
            print("ì˜ëª»ëœ ì…ë ¥.")
            return

        if not token:
            print("í† í° ì—†ìŒ. ì¢…ë£Œ.")
            return

        authenticated = await authenticate_with_token(websocket, token)
        if not authenticated:
            return

        # Enter a loop to handle additional requests
        while True:
            print("1. API State Request")
            print("2. Get Current Model")
            print("3. Get the value for all Live2D parameters in the current model")
            print("4. Chat with character")  # Modified this option
            print("5. Heart Eyes On/Off")
            print("6. Eyes Cry On/Off")
            print("7. Angry sign On/Off")
            print("8. Shock sign On/Off")
            print("9. Anim shake On/Off")
            print("0. Exit")

            choice = input("Enter your choice (1/2/3/4/5/6/7/8/9/0): ")

            if choice == "1":
                api_request = {
                    "apiName": "VTubeStudioPublicAPI",
                    "apiVersion": "1.0",
                    "requestID": "SomeID",
                    "messageType": "APIStateRequest",
                }
                await send_api_request(api_request, websocket)

            elif choice == "2":
                api_request = {
                    "apiName": "VTubeStudioPublicAPI",
                    "apiVersion": "1.0",
                    "requestID": "SomeID",
                    "messageType": "CurrentModelRequest",
                }
                await send_api_request(api_request, websocket)

            elif choice == "3":
                api_request = {
                    "apiName": "VTubeStudioPublicAPI",
                    "apiVersion": "1.0",
                    "requestID": "SomeID",
                    "messageType": "Live2DParameterListRequest",
                }
                await send_api_request(api_request, websocket)

            elif choice == "4":
                # Modified to use our new chat function
                await chat_with_character(websocket)

            elif choice == "5":
                api_request = {
                    "apiName": "VTubeStudioPublicAPI",
                    "apiVersion": "1.0",
                    "requestID": "SomeID",
                    "messageType": "HotkeyTriggerRequest",
                    "data": {
                        "hotkeyID": "995d790e63e940e78b43b717ef214756",
                    },
                }
                await send_api_request(api_request, websocket)

            elif choice == "6":
                api_request = {
                    "apiName": "VTubeStudioPublicAPI",
                    "apiVersion": "1.0",
                    "requestID": "SomeID",
                    "messageType": "HotkeyTriggerRequest",
                    "data": {
                        "hotkeyID": "ac728f4052064ae6b05c58e9290167f9",
                    },
                }
                await send_api_request(api_request, websocket)

            elif choice == "7":
                api_request = {
                    "apiName": "VTubeStudioPublicAPI",
                    "apiVersion": "1.0",
                    "requestID": "SomeID",
                    "messageType": "HotkeyTriggerRequest",
                    "data": {
                        "hotkeyID": "23b57c8036f240779608c9d246c7716c",
                    },
                }
                await send_api_request(api_request, websocket)

            elif choice == "8":
                api_request = {
                    "apiName": "VTubeStudioPublicAPI",
                    "apiVersion": "1.0",
                    "requestID": "SomeID",
                    "messageType": "HotkeyTriggerRequest",
                    "data": {
                        "hotkeyID": "8050ec17c3ee41a1b824ee1815407022",
                    },
                }
                await send_api_request(api_request, websocket)

            elif choice == "9":
                api_request = {
                    "apiName": "VTubeStudioPublicAPI",
                    "apiVersion": "1.0",
                    "requestID": "SomeID",
                    "messageType": "HotkeyTriggerRequest",
                    "data": {
                        "hotkeyID": "0415de2ef49e4943a9bfbe5dfeda3452",
                    },
                }
                await send_api_request(api_request, websocket)

            elif choice == "0":
                print("Exiting the loop.")
                break

            else:
                print("Invalid choice. Please enter 1, 2, 3, 4, 5, or 0.")


if __name__ == "__main__":
    asyncio.get_event_loop().run_until_complete(main())
